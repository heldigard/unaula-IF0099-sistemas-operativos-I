---
marp: true
theme: default
paginate: true
header: 'IF0099 - Sistemas Operativos I | Unidad 8'
footer: 'UNAULA - IngenierÃ­a InformÃ¡tica - 2026-I'
---

# Clase 13: Sistemas Distribuidos

<style>
section {
  font-size: 24px;
}
img {
  max-width: 70% !important;
  max-height: 50vh !important;
  object-fit: contain !important;
  height: auto !important;
  display: block !important;
  margin: 0 auto !important;
}
section {
  font-size: 20px;
  overflow: hidden;
}
section h1 {
  font-size: 1.8em;
}
section h2 {
  font-size: 1.4em;
}
section h3 {
  font-size: 1.2em;
}
section ul, section ol {
  font-size: 0.9em;
  margin-left: 1em;
}
section li {
  margin-bottom: 0.3em;
}
section pre {
  font-size: 0.7em;
  max-height: 60vh;
  overflow-y: auto;
}
section code {
  font-size: 0.85em;
}
section p {
  margin: 0.5em 0;
}
/* Estilos para tablas responsivas */
section table {
  width: 100%;
  max-width: 100%;
  font-size: 0.85em;
  border-collapse: collapse;
  margin: 0.5em auto;
  table-layout: auto;
}
section th {
  background-color: #1e40af;
  color: white;
  padding: 0.4em 0.6em;
  text-align: left;
  font-size: 0.9em;
  border: 1px solid #ddd;
}
section td {
  padding: 0.4em 0.6em;
  border: 1px solid #ddd;
  vertical-align: top;
  word-wrap: break-word;
  font-size: 0.85em;
}
section tbody tr:nth-child(even) {
  background-color: #f8f9fa;
}
section tbody tr:hover {
  background-color: #e9ecef;
}
/* Asegurar que el contenido no desborde */
section {
  padding: 1em 2em;
  box-sizing: border-box;
}
/* Responsividad para tablas anchas */
@media screen and (max-width: 1280px) {
  section table {
    font-size: 0.75em;
  }
  section th, section td {
    padding: 0.3em 0.4em;
  }
}
</style>

---

## Objetivos de la Clase

Al finalizar esta clase, el estudiante serÃ¡ capaz de:

1. **Definir** quÃ© es un sistema distribuido
2. **Identificar** ventajas y desafÃ­os principales
3. **Reconocer** modelos de comunicaciÃ³n y coordinaciÃ³n
4. **Relacionar** SO con servicios distribuidos

**DuraciÃ³n:** 90 minutos

---

## Agenda

1. Conceptos bÃ¡sicos (20 min)
2. Tipos de sistemas distribuidos (15 min)
3. Retos: sincronizaciÃ³n, fallos, consistencia (25 min)
4. Casos reales (20 min)
5. Actividad (10 min)

---

## 1. Â¿QuÃ© es un Sistema Distribuido?

### Ejemplos cotidianos de sistemas distribuidos

**En tu dÃ­a a dÃ­a:**
- ğŸŒ **Google:** Miles de servidores trabajando juntos
- ğŸ“± **WhatsApp:** Mensajes sincronizados en mÃºltiples dispositivos
- ğŸ® **Juegos online:** Jugadores conectados desde todo el mundo
- ğŸ’° **Blockchain:** Bitcoin y criptomonedas
- ğŸ“§ **Email:** Servidores distribuidos globalmente

ğŸ’¡ **La nube (AWS, Azure, Google Cloud) son sistemas distribuidos masivos.**


> Conjunto de computadores independientes que se presentan como un solo sistema coherente.

```
Cliente â”€â”€â–º Nodo A â”€â”€â–º Nodo B
           â”‚            â”‚
           â””â”€â”€â–º Nodo C â”€â”˜
```

CaracterÃ­sticas:
- Concurrencia
- Falta de reloj global
- Fallos parciales

---

## 2. Tipos Comunes

| Tipo | Ejemplo |
| ------ | --------- |
| **ClÃºster** | HPC, procesamiento paralelo |
| **Cliente-Servidor** | Bases de datos, apps web |
| **P2P** | BitTorrent, blockchain |
| **Microservicios** | Arquitecturas modernas en cloud |

---

## 3. Retos Clave

### Consistencia vs Disponibilidad
- Teorema CAP: no se pueden maximizar ambas simultÃ¡neamente

### Fallos parciales
- Un nodo puede fallar sin caer todo el sistema

### SincronizaciÃ³n
- Locks distribuidos
- ElecciÃ³n de lÃ­der

---

## 4. ComunicaciÃ³n

| Modelo | DescripciÃ³n |
| -------- | ------------- |
| **RPC** | Llamadas remotas como si fueran locales |
| **MensajerÃ­a** | Cola y eventos (Kafka, RabbitMQ) |
| **REST** | APIs HTTP |

---

## Casos Reales

- **Google File System / HDFS**: almacenamiento distribuido
- **Kubernetes**: orquestaciÃ³n de contenedores
- **CDN**: distribuciÃ³n global de contenido

---

## Actividad (10 min)

En parejas:
1. Dar un ejemplo de sistema distribuido en su vida diaria
2. Identificar 2 retos y 1 ventaja

---

## Resumen

| Concepto | Idea clave |
| ---------- | ------------ |
| **Distribuido** | Varios nodos = un solo sistema |
| **Retos** | Fallos parciales, consistencia, sincronizaciÃ³n |
| **Modelos** | RPC, mensajerÃ­a, REST |

---

## PrÃ³xima Clase

### Clase 14: Programas de AplicaciÃ³n e Interfaces

- Llamadas al sistema
- APIs del SO
- Ejemplos en Linux/Windows

**Â¡Nos vemos!**


---
### 1. Kubernetes - OrquestaciÃ³n de Contenedores



**Â¿QuÃ© es?**
Sistema distribuido para gestionar contenedores Docker en mÃºltiples mÃ¡quinas.

**Arquitectura:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          KUBERNETES CLUSTER             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Master     â”‚    â”‚    etcd      â”‚  â”‚
â”‚  â”‚   (Control   â”‚â—„â”€â”€â–ºâ”‚  (Registro   â”‚  â”‚
â”‚  â”‚    Plane)    â”‚    â”‚ Distribuido) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                               â”‚
â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚         â”‚        â”‚        â”‚       â”‚
â”‚  â”Œâ”€â–¼â”€â”€â”   â”Œâ”€â–¼â”€â”€â”  â”Œâ”€â–¼â”€â”€â”  â”Œâ”€â–¼â”€â”€â”     â”‚
â”‚  â”‚Nodeâ”‚   â”‚Nodeâ”‚  â”‚Nodeâ”‚  â”‚Nodeâ”‚     â”‚
â”‚  â”‚  1 â”‚   â”‚  2 â”‚  â”‚  3 â”‚  â”‚  4 â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜     â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---
### 1. Kubernetes - OrquestaciÃ³n de Contenedores (ContinuaciÃ³n)

**Componentes Principales del Cluster:**

- **Master Node (Control Plane):**
  - **API Server:** Punto de entrada para todas las operaciones
  - **Scheduler:** Decide en quÃ© nodo ejecutar cada pod
  - **Controller Manager:** Mantiene el estado deseado del cluster
  - **etcd:** Base de datos distribuida con toda la configuraciÃ³n

- **Worker Nodes:**
  - **Kubelet:** Agente que ejecuta containers en cada nodo
  - **Kube-proxy:** Maneja networking y balanceo de carga
  - **Container Runtime:** Docker, containerd, etc.

**CaracterÃ­sticas Distribuidas:**
- **ReplicaciÃ³n:** Copia automÃ¡tica de pods en mÃºltiples nodos
- **Balanceo de carga:** Distribuye trÃ¡fico entre rÃ©plicas
- **Auto-recuperaciÃ³n:** Si un nodo falla, mueve pods a otro nodo
- **Escalado horizontal:** AÃ±ade/quita nodos dinÃ¡micamente

---

### 2. Apache Cassandra - Base de Datos Distribuida

**Arquitectura sin Maestro:**
```
     Nodo A
     /    \
   /        \
Nodo B â”€â”€â”€ Nodo C
   \        /
    \      /
     Nodo D
```

**CaracterÃ­sticas:**
- **Sin single point of failure:** Todos los nodos son iguales
- **ReplicaciÃ³n configurable:** Datos en N nodos (ej: N=3)
- **Consistencia eventual:** Escribe en algunos, lee de algunos
- **Particionamiento:** Hash ring divide datos entre nodos

**Ejemplo de Escritura:**

```python
# Cliente escribe "usuario123" con RF=3 (Replication Factor)
cassandra.insert("usuarios", "usuario123", datos)

# Cassandra automÃ¡ticamente:
# 1. Calcula hash(usuario123) = 0x8A3F...
# 2. Ubica en el anillo: Nodo B es responsable
# 3. Replica en Nodos C y D (siguientes en el anillo)
# 4. Escritura exitosa si 2 de 3 nodos confirman (Quorum)
```

---
### 3. Google File System (GFS) / HDFS

**Problema que resuelve:**
Almacenar archivos de **petabytes** en miles de mÃ¡quinas comunes (no servidores caros).

**Arquitectura:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           GFS MASTER                   â”‚
â”‚  â€¢ Almacena metadata (nombres, etc)   â”‚
â”‚  â€¢ Coordina operaciones                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚        â”‚        â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”
â”‚Chunk  â”‚ â”‚Chunk â”‚ â”‚Chunk â”‚ â”‚Chunk â”‚
â”‚Server â”‚ â”‚Serverâ”‚ â”‚Serverâ”‚ â”‚Serverâ”‚
â”‚   1   â”‚ â”‚  2   â”‚ â”‚  3   â”‚ â”‚  4   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
  Datos    Datos    Datos    Datos
```

---
### 3. Google File System (GFS) / HDFS (ContinuaciÃ³n)

**Componentes del Sistema:**

- **Master (NameNode en HDFS):**
  - Almacena metadata: nombres de archivos, permisos, ubicaciÃ³n de chunks
  - NO almacena datos reales (solo metadatos)
  - Single point of failure (mitigado con Secondary Master)

- **Chunk Servers (DataNodes en HDFS):**
  - Almacenan los datos reales
  - Reportan su estado al Master periÃ³dicamente
  - Se comunican directamente con los clientes

**TamaÃ±o de Chunks:**
- GFS: 64 MB (mucho mÃ¡s grande que bloques de filesystems tradicionales)
- HDFS: 128 MB (por defecto)
- Â¿Por quÃ© tan grandes? Reduce overhead de metadata

**Proceso de Lectura:**

1. Cliente: "Quiero leer `video.mp4` desde byte 1GB"
2. Master: "Los chunks estÃ¡n en Servers 2, 3, 4"
3. Cliente lee directamente de esos servidores (paralelo!)

**ReplicaciÃ³n:**
Cada chunk (64 MB) se replica en 3 servidores diferentes.
Si uno falla, Master ordena replicar desde otro.

---
### 4. Netflix - CDN Distribuido

**Problema:** Entregar video HD a 200M usuarios simultÃ¡neos sin lag.

**SoluciÃ³n: Open Connect**

```
        Usuario en MedellÃ­n
              â”‚
              â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   CDN Cache  â”‚  â† Servidor en MedellÃ­n (local)
      â”‚   Colombia   â”‚     Tiene pelÃ­culas populares
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ (si no estÃ¡)
             â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  CDN Regionalâ”‚  â† Servidor en Miami
      â”‚   LatinoamÃ©ricaâ”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ (si no estÃ¡)
             â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Origen AWS  â”‚  â† Servidor origen en Virginia
      â”‚   EE.UU.     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---
### 4. Netflix - CDN Distribuido (ContinuaciÃ³n)

**Arquitectura de Open Connect:**

**JerarquÃ­a de Caches:**
1. **Edge Cache (Nivel 1):** Servidores en ciudades grandes
   - Ubicados en ISPs locales
   - 20-40 TB de almacenamiento
   - Atienden 90% del trÃ¡fico

2. **Regional Cache (Nivel 2):** Servidores regionales
   - Ubicados en puntos de intercambio de internet
   - 100+ TB de almacenamiento
   - Respaldan a los edge caches

3. **Origin Servers:** AWS Cloud
   - AlmacÃ©n completo del catÃ¡logo
   - Codifican video en mÃºltiples resoluciones/bitrates

**Algoritmo de Entrega:**
```
Usuario solicita pelÃ­cula
    â†“
Verificar: Â¿EstÃ¡ en Edge Cache?
    â†“ SÃ­ â†’ Entregar desde Edge (<50ms)
    â†“ No
Verificar: Â¿EstÃ¡ en Regional Cache?
    â†“ SÃ­ â†’ Entregar desde Regional (<200ms)
    â†“ No
Obtener desde Origin + Cachear en Regional/Edge
```

**Optimizaciones:**
- **Pre-caching:** PelÃ­culas populares se cargan en caches durante la noche
- **Smart routing:** DNS inteligente dirige al servidor mÃ¡s cercano
- **Adaptive streaming:** Calidad se ajusta segÃºn ancho de banda

**MÃ©tricas Reales:**
- **90% del trÃ¡fico** se sirve desde cache local
- **Latencia promedio:** <50 ms
- **Throughput:** 100 Gbps por servidor edge

---

### 5. WhatsApp - Sistema de MensajerÃ­a Distribuido

**Arquitectura (Simplificada):**

```
Usuario A (Colombia)             Usuario B (JapÃ³n)
      â”‚                                â”‚
      â†“                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Servidor â”‚                    â”‚ Servidor â”‚
â”‚ Colombia â”‚â”€â”€â”€â”€SincronizaciÃ³nâ”€â”€â”‚  JapÃ³n   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â–º Ejabberd â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              (Protocolo XMPP)
```

**TecnologÃ­a:**
- **Erlang:** Lenguaje diseÃ±ado para sistemas distribuidos
- **Actor model:** Cada conversaciÃ³n es un proceso ligero
- **50M mensajes/segundo** en infraestructura distribuida

**GarantÃ­as:**
- **Entrega garantizada:** Aunque el receptor estÃ© offline
- **Ordenamiento:** Mensajes llegan en orden enviado
- **Cifrado E2E:** Solo emisor y receptor descifran

---

### ğŸ“Š ComparaciÃ³n de Estrategias

| Sistema | Consistencia | Disponibilidad | Particionamiento |
|---------|--------------|----------------|------------------|
| **Kubernetes** | Fuerte (etcd) | Alta | SÃ­ |
| **Cassandra** | Eventual | Muy alta | SÃ­ |
| **GFS/HDFS** | Eventual | Alta | SÃ­ |
| **Netflix CDN** | Eventual | Muy alta | GeogrÃ¡fico |
| **WhatsApp** | Fuerte | Alta | Por usuario |

---

### ğŸ’¡ Teorema CAP en PrÃ¡ctica

**CAP:** Solo puedes garantizar 2 de 3 (Consistency, Availability, Partition tolerance)

**Elecciones:**
- **Cassandra:** AP (Disponibilidad + Particionamiento) â†’ Eventual consistency
- **etcd (Kubernetes):** CP (Consistencia + Particionamiento) â†’ Menor disponibilidad
- **DNS:** AP â†’ Por eso a veces ves info desactualizada

---

## ğŸ’» Actividad PrÃ¡ctica: Explorar Kubernetes Local

### Requisitos: Docker Desktop + Habilitar Kubernetes

```bash
# 1. Verificar que Kubernetes estÃ© corriendo
kubectl version --short

# 2. Crear un deployment simple (3 rÃ©plicas)
kubectl create deployment hello --image=nginx --replicas=3

# 3. Ver los pods distribuidos
kubectl get pods -o wide
# Observa en quÃ© nodos estÃ¡n

# 4. Exponer el servicio
kubectl expose deployment hello --port=80 --type=LoadBalancer

# 5. Simular falla: eliminar un pod
kubectl delete pod <nombre-de-un-pod>

# 6. Ver auto-recuperaciÃ³n
kubectl get pods -w  # (-w = watch en tiempo real)
# Kubernetes automÃ¡ticamente crea un nuevo pod!

# 7. Escalar horizontalmente
kubectl scale deployment hello --replicas=5

# 8. Limpiar
kubectl delete deployment hello
kubectl delete service hello
```

### Preguntas de ReflexiÃ³n:

1. Â¿QuÃ© pasÃ³ cuando eliminaste un pod?
2. Â¿En cuÃ¡nto tiempo se creÃ³ el reemplazo?
3. Â¿Los pods estÃ¡n en el mismo nodo o distribuidos?
4. Â¿CÃ³mo garantiza Kubernetes alta disponibilidad?

### Tiempo estimado: 45 minutos

---
